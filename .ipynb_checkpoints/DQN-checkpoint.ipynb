{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49b279f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:52:25.352088Z",
     "start_time": "2021-05-18T16:52:25.348620Z"
    }
   },
   "outputs": [],
   "source": [
    "from carts_poles import CartsPolesEnv\n",
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b2e4237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T17:01:01.299913Z",
     "start_time": "2021-05-18T17:01:01.295720Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-a689c0f12e3c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-a689c0f12e3c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    global FLAGS = {\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Hyper parms!!\n",
    "FLAGS = {\n",
    "    \"gamma\" : 0.99,\n",
    "    \n",
    "    \"n_episode\" : 1000,\n",
    "\n",
    "    \"batch_size\" : 64,\n",
    "\n",
    "    \"hidden_dim\" : 12,\n",
    "\n",
    "    \"capacity\" : 50000,\n",
    "\n",
    "    \"max_episode\" : 50,\n",
    "\n",
    "    \"min_eps\" : 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec1743e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:56:27.856006Z",
     "start_time": "2021-05-18T16:56:27.813684Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"DQN Network\n",
    "        Args:\n",
    "            input_dim (int): `state` dimension.\n",
    "                `state` is 2-D tensor of shape (n, input_dim)\n",
    "            output_dim (int): Number of actions.\n",
    "                Q_value is 2-D tensor of shape (n, output_dim)\n",
    "            hidden_dim (int): Hidden dimension in fc layer\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.final = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns a Q_value\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 2-D tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Replay memory class\n",
    "        Args:\n",
    "            capacity (int): Max size of this memory\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.cursor = 0\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self,\n",
    "             state: np.ndarray,\n",
    "             action: int,\n",
    "             reward: int,\n",
    "             next_state: np.ndarray,\n",
    "             done: bool) -> None:\n",
    "        \"\"\"Creates `Transition` and insert\n",
    "        Args:\n",
    "            state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            action (int): action index (0 <= action < output_dim)\n",
    "            reward (int): reward value\n",
    "            next_state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            done (bool): whether this state was last step\n",
    "        \"\"\"\n",
    "        if len(self) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.cursor] = Transition(state,\n",
    "                                              action, reward, next_state, done)\n",
    "        self.cursor = (self.cursor + 1) % self.capacity\n",
    "\n",
    "    def pop(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Returns a minibatch of `Transition` randomly\n",
    "        Args:\n",
    "            batch_size (int): Size of mini-bach\n",
    "        Returns:\n",
    "            List[Transition]: Minibatch of `Transition`\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length \"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"Agent class that choose action and train\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden dimension\n",
    "        \"\"\"\n",
    "        self.dqn = DQN(input_dim, output_dim, hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.dqn.parameters())\n",
    "\n",
    "    def _to_variable(self, x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"torch.Variable syntax helper\n",
    "        Args:\n",
    "            x (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: torch variable\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "    def get_action(self, states: np.ndarray, eps: float) -> int:\n",
    "        \"\"\"Returns an action\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "            eps (float): ùú∫-greedy for exploration\n",
    "        Returns:\n",
    "            int: action index\n",
    "        \"\"\"\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.choice(self.output_dim)\n",
    "        else:\n",
    "            self.dqn.train(mode=False)\n",
    "            scores = self.get_Q(states)\n",
    "            _, argmax = torch.max(scores.data, 1)\n",
    "            return int(argmax.numpy())\n",
    "\n",
    "    def get_Q(self, states: np.ndarray) -> torch.FloatTensor:\n",
    "        \"\"\"Returns `Q-value`\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D Tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.FloatTensor: 2-D Tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        states = self._to_variable(states.reshape(-1, self.input_dim))\n",
    "        self.dqn.train(mode=False)\n",
    "        return self.dqn(states)\n",
    "\n",
    "    def train(self, Q_pred: torch.FloatTensor, Q_true: torch.FloatTensor) -> float:\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        Args:\n",
    "            Q_pred (torch.FloatTensor): Predicted value by the network,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "            Q_true (torch.FloatTensor): Target value obtained from the game,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "        Returns:\n",
    "            float: loss value\n",
    "        \"\"\"\n",
    "        self.dqn.train(mode=True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss_fn(Q_pred, Q_true)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_helper(agent: Agent, minibatch: List[Transition], gamma: float) -> float:\n",
    "    \"\"\"Prepare minibatch and train them\n",
    "    Args:\n",
    "        agent (Agent): Agent has `train(Q_pred, Q_true)` method\n",
    "        minibatch (List[Transition]): Minibatch of `Transition`\n",
    "        gamma (float): Discount rate of Q_target\n",
    "    Returns:\n",
    "        float: Loss value\n",
    "    \"\"\"\n",
    "    states = np.vstack([x.state for x in minibatch])\n",
    "    actions = np.array([x.action for x in minibatch])\n",
    "    rewards = np.array([x.reward for x in minibatch])\n",
    "    next_states = np.vstack([x.next_state for x in minibatch])\n",
    "    done = np.array([x.done for x in minibatch])\n",
    "\n",
    "    Q_predict = agent.get_Q(states)\n",
    "    Q_target = Q_predict.clone().data.numpy()\n",
    "    Q_target[np.arange(len(Q_target)), actions] = rewards + gamma * np.max(agent.get_Q(next_states).data.numpy(), axis=1) * ~done\n",
    "    Q_target = agent._to_variable(Q_target)\n",
    "\n",
    "    return agent.train(Q_predict, Q_target)\n",
    "\n",
    "\n",
    "def play_episode(env: gym.Env,\n",
    "                 agent: Agent,\n",
    "                 replay_memory: ReplayMemory,\n",
    "                 eps: float,\n",
    "                 batch_size: int,\n",
    "                 render = False) -> int:\n",
    "    \"\"\"Play an epsiode and train\n",
    "    Args:\n",
    "        env (gym.Env): gym environment (CartPole-v0)\n",
    "        agent (Agent): agent will train and get action\n",
    "        replay_memory (ReplayMemory): trajectory is saved here\n",
    "        eps (float): ùú∫-greedy for exploration\n",
    "        batch_size (int): batch size\n",
    "    Returns:\n",
    "        int: reward earned in this episode\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s, eps)\n",
    "        s2, r, done, info = env.step(a)\n",
    "\n",
    "        if render == True:\n",
    "            env.render()\n",
    "\n",
    "        total_reward += r\n",
    "\n",
    "        if done:\n",
    "            r = -1\n",
    "        replay_memory.push(s, a, r, s2, done)\n",
    "\n",
    "        if len(replay_memory) > batch_size:\n",
    "\n",
    "            minibatch = replay_memory.pop(batch_size)\n",
    "            train_helper(agent, minibatch, FLAGS.gamma)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def get_env_dim(env: gym.Env) -> Tuple[int, int]:\n",
    "    \"\"\"Returns input_dim & output_dim\n",
    "    Args:\n",
    "        env (gym.Env): gym Environment (CartPole-v0)\n",
    "    Returns:\n",
    "        int: input_dim\n",
    "        int: output_dim\n",
    "    \"\"\"\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    return input_dim, output_dim\n",
    "\n",
    "\n",
    "def epsilon_annealing(epsiode: int, max_episode: int, min_eps: float) -> float:\n",
    "    \"\"\"Returns ùú∫-greedy\n",
    "    1.0---|\\\n",
    "          | \\\n",
    "          |  \\\n",
    "    min_e +---+------->\n",
    "              |\n",
    "              max_episode\n",
    "    Args:\n",
    "        epsiode (int): Current episode (0<= episode)\n",
    "        max_episode (int): After max episode, ùú∫ will be `min_eps`\n",
    "        min_eps (float): ùú∫ will never go below this value\n",
    "    Returns:\n",
    "        float: ùú∫ value\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_eps - 1.0) / max_episode\n",
    "    return max(slope * epsiode + 1.0, min_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3cd430f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:56:48.831833Z",
     "start_time": "2021-05-18T16:56:48.791194Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FLAGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cb6bec828c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Episode: {:5}] Reward: {:5} ùú∫-greedy: {:5.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4610b1f92942>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(env, agent, replay_memory, eps, batch_size, render)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mtrain_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple(\"Transition\",\n",
    "                        field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "env = CartsPolesEnv()\n",
    "rewards = np.zeros(n_episode)\n",
    "input_dim, output_dim = get_env_dim(env)\n",
    "agent = Agent(input_dim, output_dim, hidden_dim)\n",
    "replay_memory = ReplayMemory(capacity)\n",
    "\n",
    "for i in range(n_episode):\n",
    "    render = False\n",
    "    eps = epsilon_annealing(i, max_episode, min_eps)\n",
    "\n",
    "    if (i % 50 == 0): render = False\n",
    "\n",
    "    r = play_episode(env, agent, replay_memory, eps,batch_size,render)\n",
    "    print(\"[Episode: {:5}] Reward: {:5} ùú∫-greedy: {:5.2f}\".format(i + 1, r, eps))\n",
    "\n",
    "    rewards[i] = r\n",
    "\n",
    "    # if len(rewards) == rewards.maxlen:\n",
    "    #     if np.mean(rewards) >= 200:\n",
    "    #         print(\"Game cleared in {} games with {}\".format(i + 1, np.mean(rewards)))\n",
    "    #         break\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.plot(range(0,n_episode),rewards)\n",
    "# ax2.plot(range(0,n_episode),td_error,c='g')\n",
    "plt.title(\"DQN Episode Length vs Episode\")\n",
    "# ax2.title.set_text(\"TD Error Convergence\")\n",
    "# fig.suptitle('DQN Control')\n",
    "# ax1.set_xscale('log')\n",
    "# ax2.set_xscale('log')\n",
    "plt.grid()\n",
    "# ax2.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55a178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
